{"cells":[{"cell_type":"markdown","source":["## Section 1) Mount Azure storage blob container"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"471d9060-044d-4570-ab66-f37affa0b84b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Mount Blob to DBFS\ndbutils.fs.mount(\n  source = \"wasbs://datalake@dsba6190storageepsilon.blob.core.windows.net/\",\n  mount_point = \"/mnt/epsilon/\",\n  extra_configs = {\"fs.azure.account.key.dsba6190storageepsilon.blob.core.windows.net\": \"HB87PDIxoCmvJYrC5JqMzEic3ySYwEll03NoGfRtJiviX6oQmS1Khnhz4wjIP30r41vIGIn5Cqo9+AStVPZD5Q==\"})"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"92e5621b-6b31-4743-87c1-3182956185a9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-3128781507651148>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Mount Blob to DBFS\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m dbutils.fs.mount(\n\u001B[0m\u001B[1;32m      3\u001B[0m   \u001B[0msource\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"wasbs://datalake@dsba6190storageepsilon.blob.core.windows.net/\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m   \u001B[0mmount_point\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"/mnt/epsilon/\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m   extra_configs = {\"fs.azure.account.key.dsba6190storageepsilon.blob.core.windows.net\": \"HB87PDIxoCmvJYrC5JqMzEic3ySYwEll03NoGfRtJiviX6oQmS1Khnhz4wjIP30r41vIGIn5Cqo9+AStVPZD5Q==\"})\n\n\u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py\u001B[0m in \u001B[0;36mf_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m                     \u001B[0mexc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__context__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    361\u001B[0m                     \u001B[0mexc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__cause__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 362\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mexc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    363\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    364\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mf_with_exception_handling\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o395.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/epsilon; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/epsilon\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1004)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1030)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:541)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:398)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:396)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:393)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:66)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:441)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:426)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:66)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:550)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:66)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:541)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:511)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:66)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:130)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1024)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/epsilon\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:490)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$1(MetadataManager.scala:855)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:636)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:844)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:498)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:120)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:347)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:306)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:119)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:116)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:24)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:115)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:958)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:958)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:874)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2(JettyServer.scala:503)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2$adapted(JettyServer.scala:478)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:390)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionTags(ActivityContextFactory.scala:60)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:390)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:171)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:478)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:375)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:80)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n","errorSummary":"java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/epsilon; nested exception is: ","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-3128781507651148>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Mount Blob to DBFS\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m dbutils.fs.mount(\n\u001B[0m\u001B[1;32m      3\u001B[0m   \u001B[0msource\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"wasbs://datalake@dsba6190storageepsilon.blob.core.windows.net/\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m   \u001B[0mmount_point\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"/mnt/epsilon/\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m   extra_configs = {\"fs.azure.account.key.dsba6190storageepsilon.blob.core.windows.net\": \"HB87PDIxoCmvJYrC5JqMzEic3ySYwEll03NoGfRtJiviX6oQmS1Khnhz4wjIP30r41vIGIn5Cqo9+AStVPZD5Q==\"})\n\n\u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py\u001B[0m in \u001B[0;36mf_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m                     \u001B[0mexc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__context__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    361\u001B[0m                     \u001B[0mexc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__cause__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 362\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mexc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    363\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    364\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mf_with_exception_handling\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o395.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/epsilon; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/epsilon\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1004)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1030)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:541)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:636)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:398)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:147)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:396)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:393)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:66)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:441)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:426)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:66)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:631)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:550)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:66)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:541)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:511)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:66)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:130)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1024)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/epsilon\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:490)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$1(MetadataManager.scala:855)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:636)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:844)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:498)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:120)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:347)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:306)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:119)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:146)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:116)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:550)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:645)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:666)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:640)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:559)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:550)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:520)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:24)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:115)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:958)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:958)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:874)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2(JettyServer.scala:503)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$2$adapted(JettyServer.scala:478)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:390)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:407)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:405)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:402)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:450)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:435)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionTags(ActivityContextFactory.scala:60)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:390)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:171)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:478)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:375)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:80)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n"]}}],"execution_count":0},{"cell_type":"code","source":["#dbutils.fs.ls(\"/mnt/\")\ndisplay(dbutils.fs.mounts())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"63ca8dd8-2e6f-40e2-8a71-dedd5de47eb3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["/mnt/Delta/","wasbs://datalake@dsba6190storagedelta.blob.core.windows.net/",""],["/databricks-datasets","databricks-datasets",""],["/mnt/epsilon/","wasbs://datalake@dsba6190storageepsilon.blob.core.windows.net/",""],["/mnt/delta-sai/","wasbs://datalake@dsba6190storagedelta.blob.core.windows.net/",""],["/mnt/gamma/","wasbs://datawarehouse@dsba6190storagegamma.blob.core.windows.net/",""],["/mnt/zeta/","wasbs://datalake@dsba6190storageks.blob.core.windows.net/",""],["/databricks/mlflow-tracking","databricks/mlflow-tracking",""],["/databricks-results","databricks-results",""],["/mnt/<beta>/","wasbs://data@dsba6190storagebeta.blob.core.windows.net/",""],["/mnt/beta/","wasbs://data@dsba6190storagebeta.blob.core.windows.net/",""],["/databricks/mlflow-registry","databricks/mlflow-registry",""],["/mnt/dsba6190-gamma-rg/","wasbs://datawarehouse@dsba6190storagegamma.blob.core.windows.net/",""],["/mnt/DELTA/","wasbs://datalake@dsba6190storagedelta.blob.core.windows.net/",""],["/mnt/instructor/","wasbs://datalake@dsba6190storageinst.blob.core.windows.net/",""],["/mnt/alpha/","wasbs://datalake@dsba6190storagealpha.blob.core.windows.net/",""],["/mnt/delta/","wasbs://datalake@dsba6190storagedelta.blob.core.windows.net/",""],["/","DatabricksRoot",""]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"mountPoint","type":"\"string\"","metadata":"{}"},{"name":"source","type":"\"string\"","metadata":"{}"},{"name":"encryptionType","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mountPoint</th><th>source</th><th>encryptionType</th></tr></thead><tbody><tr><td>/mnt/Delta/</td><td>wasbs://datalake@dsba6190storagedelta.blob.core.windows.net/</td><td></td></tr><tr><td>/databricks-datasets</td><td>databricks-datasets</td><td></td></tr><tr><td>/mnt/epsilon/</td><td>wasbs://datalake@dsba6190storageepsilon.blob.core.windows.net/</td><td></td></tr><tr><td>/mnt/delta-sai/</td><td>wasbs://datalake@dsba6190storagedelta.blob.core.windows.net/</td><td></td></tr><tr><td>/mnt/gamma/</td><td>wasbs://datawarehouse@dsba6190storagegamma.blob.core.windows.net/</td><td></td></tr><tr><td>/mnt/zeta/</td><td>wasbs://datalake@dsba6190storageks.blob.core.windows.net/</td><td></td></tr><tr><td>/databricks/mlflow-tracking</td><td>databricks/mlflow-tracking</td><td></td></tr><tr><td>/databricks-results</td><td>databricks-results</td><td></td></tr><tr><td>/mnt/<beta>/</td><td>wasbs://data@dsba6190storagebeta.blob.core.windows.net/</td><td></td></tr><tr><td>/mnt/beta/</td><td>wasbs://data@dsba6190storagebeta.blob.core.windows.net/</td><td></td></tr><tr><td>/databricks/mlflow-registry</td><td>databricks/mlflow-registry</td><td></td></tr><tr><td>/mnt/dsba6190-gamma-rg/</td><td>wasbs://datawarehouse@dsba6190storagegamma.blob.core.windows.net/</td><td></td></tr><tr><td>/mnt/DELTA/</td><td>wasbs://datalake@dsba6190storagedelta.blob.core.windows.net/</td><td></td></tr><tr><td>/mnt/instructor/</td><td>wasbs://datalake@dsba6190storageinst.blob.core.windows.net/</td><td></td></tr><tr><td>/mnt/alpha/</td><td>wasbs://datalake@dsba6190storagealpha.blob.core.windows.net/</td><td></td></tr><tr><td>/mnt/delta/</td><td>wasbs://datalake@dsba6190storagedelta.blob.core.windows.net/</td><td></td></tr><tr><td>/</td><td>DatabricksRoot</td><td></td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Section 1b) Read in Your Data\n\n### Description of the Data: \nThe dataset is credited to Ronny Kohavi and Barry Becker and was drawn from the 1994 United States Census Bureau data and involves using personal details such as education level to predict whether an individual will earn more or less than $50,000 per year.\n### Data Source: \nhttps://archive.ics.uci.edu/ml/datasets/adult"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5977c071-848d-4319-920e-e481457a2e24","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = sqlContext.read.format('csv') \\\n                    .options(header='true', inferSchema='true', delimiter= ',') \\\n                    .load('/mnt/epsilon/adult.csv')\n\n\n#df.describe()\ndf=df.withColumnRenamed(\"occupation6\",\"occupation\")\ndf=df.withColumnRenamed(\"occupation7\",\"relationship\")\ndf.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"16c598de-69a6-47c6-abc5-fa0d49be0eeb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"+---+---------+------+----------+-------------+--------------+------------------+--------------+------+---+------------+------------+--------------+--------------+------+\n|age|workclass|fnlwgt| education|education-num|marital-status|        occupation|  relationship|  race|sex|capital-gain|capital-loss|hours-per-week|native-country|Salary|\n+---+---------+------+----------+-------------+--------------+------------------+--------------+------+---+------------+------------+--------------+--------------+------+\n| 39|     Govt| 77516| Bachelors|           13| Never-married|      Adm-clerical| Not-in-family| White|  0|        2174|           0|            40| United-States|     1|\n| 50| employed| 83311| Bachelors|           13|       Married|   Exec-managerial|       Husband| White|  0|           0|           0|            13| United-States|     1|\n| 38|  Private|215646|   HS-grad|            9|      Divorced| Handlers-cleaners| Not-in-family| White|  0|           0|           0|            40| United-States|     1|\n| 53|  Private|234721| schooling|            7|       Married| Handlers-cleaners|       Husband| Black|  0|           0|           0|            40| United-States|     1|\n| 28|  Private|338409| Bachelors|           13|       Married|    Prof-specialty|          Wife| Black|  1|           0|           0|            40|          Cuba|     1|\n+---+---------+------+----------+-------------+--------------+------------------+--------------+------+---+------------+------------+--------------+--------------+------+\nonly showing top 5 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------+------+----------+-------------+--------------+------------------+--------------+------+---+------------+------------+--------------+--------------+------+\n|age|workclass|fnlwgt| education|education-num|marital-status|        occupation|  relationship|  race|sex|capital-gain|capital-loss|hours-per-week|native-country|Salary|\n+---+---------+------+----------+-------------+--------------+------------------+--------------+------+---+------------+------------+--------------+--------------+------+\n| 39|     Govt| 77516| Bachelors|           13| Never-married|      Adm-clerical| Not-in-family| White|  0|        2174|           0|            40| United-States|     1|\n| 50| employed| 83311| Bachelors|           13|       Married|   Exec-managerial|       Husband| White|  0|           0|           0|            13| United-States|     1|\n| 38|  Private|215646|   HS-grad|            9|      Divorced| Handlers-cleaners| Not-in-family| White|  0|           0|           0|            40| United-States|     1|\n| 53|  Private|234721| schooling|            7|       Married| Handlers-cleaners|       Husband| Black|  0|           0|           0|            40| United-States|     1|\n| 28|  Private|338409| Bachelors|           13|       Married|    Prof-specialty|          Wife| Black|  1|           0|           0|            40|          Cuba|     1|\n+---+---------+------+----------+-------------+--------------+------------------+--------------+------+---+------------+------------+--------------+--------------+------+\nonly showing top 5 rows\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Section 2) Shaping the Data for Machine Learning\nTransform the data for use in machine learning. This includes One Hot Encoding, Vectorization, etc."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2e382165-df32-4c0e-b5c7-c41ce14e76c1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\n#Define which columns are numerical versus categorical\n\nlabel = \"Salary\"\ncategoricalColumns = [\"workclass\",\"education\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\"]\n\nnumericalColumns = [\"age\",\"fnlwgt\",\"education-num\",\"capital-gain\",\"capital-loss\",\"hours-per-week\"]\n\n#categoricalColumnsclassVec = [\"col1classVec\",\n#                              \"col2classVec\"]\ncategoricalColumnsclassVec = [c + \"classVec\" for c in categoricalColumns]\n\n#Set up stages\nstages = []\n\n#Index the categorical columns and perform One Hot Encoding\nfor categoricalColumn in categoricalColumns:\n  print(categoricalColumn)\n  ## Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalColumn, outputCol = categoricalColumn+\"Index\").setHandleInvalid(\"skip\")\n  ## Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalColumn+\"Index\", outputCol=categoricalColumn+\"classVec\")\n  ## Add stages\n  stages += [stringIndexer, encoder]\n\n## Convert label into label indices using the StringIndexer\nlabel_stringIndexer = StringIndexer(inputCol = label, outputCol = \"label\").setHandleInvalid(\"skip\")\nstages += [label_stringIndexer]\n\n\n##Assemble the data together as a vector\nassemblerInputs = categoricalColumnsclassVec + numericalColumns\nassembler = VectorAssembler(inputCols = assemblerInputs, outputCol = \"features\")\n\nstages += [assembler]\n\n#Scale features using Normalization\nfrom pyspark.ml.feature import StandardScaler\nscaler = StandardScaler(inputCol = \"features\",\n                        outputCol = \"scaledFeatures\",\n                        withStd = True,\n                        withMean = True)\nstages += [scaler]\n\nprepPipeline = Pipeline().setStages(stages)\npipelineModel = prepPipeline.fit(df)\ndataset = pipelineModel.transform(df)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4cbac7a0-a15a-42f8-9280-7b80be67e928","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"workclass\neducation\nmarital-status\noccupation\nrelationship\nrace\nsex\nnative-country\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["workclass\neducation\nmarital-status\noccupation\nrelationship\nrace\nsex\nnative-country\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Section 3a) Create a Machine Learning Model\n## Machine learning algorithms from Spark MLlib and train models using your data.\nLogistic Regression & Random Forest\n### Description of the Predictive Use Case: \nPredicting whether an individual will earn more or less than $50,000 per year"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"24c29131-231f-4371-9742-0d54691e8cb6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Test train split on our dataset\ntrain, test = dataset.randomSplit([0.70, 0.30], seed = 1111)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7cf26b97-5a56-4410-bb6f-119a4645a9ff","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Logistic Regression\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n#initialize logistic regression object\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n#Create a parameter grid for tuning the model\nlrparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.1, 0.5, 1.0, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10, 20, 50])\n             .build())\n#Define how you want the model to be evaluated\nlrevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName = \"areaUnderROC\")\n\n#Define the type of cross-validation you want to perform\n# Create 5-fold CrossValidator\nlrcv = CrossValidator(estimator = lr,\n                    estimatorParamMaps = lrparamGrid,\n                    evaluator = lrevaluator,\n                    numFolds = 5)\n#Fit the model to the data\nlrcvModel = lrcv.fit(train)\nprint(lrcvModel)\n#Score the testing dataset using your fitted model for evaluation purposes\nlrpredictions = lrcvModel.transform(test)\nprint('Accuracy:', lrevaluator.evaluate(lrpredictions))\nprint('AUC:', BinaryClassificationMetrics(lrpredictions['label','prediction'].rdd).areaUnderROC)\nprint('PR:', BinaryClassificationMetrics(lrpredictions['label','prediction'].rdd).areaUnderPR)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3fda0f1a-1204-4f50-bdd8-29014afaf3d5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"CrossValidatorModel_3bbae54541a0\nAccuracy: 0.9040797820470264\n/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\nAUC: 0.8080818789687665\nPR: 0.5114596617470211\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["CrossValidatorModel_3bbae54541a0\nAccuracy: 0.9040797820470264\n/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\nAUC: 0.8080818789687665\nPR: 0.5114596617470211\n"]}}],"execution_count":0},{"cell_type":"code","source":["#Naive Bayes Classifier\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n\n#Initialize Naive Bayes object\nnb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\")\n\n#Create a parameter grid for tuning the model\nnbparamGrid = (ParamGridBuilder()\n               .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n               .build())\n#Define how you want the model to be evaluated\nnbevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n\n# Create 5-fold CrossValidator\nnbcv = CrossValidator(estimator = nb,\n                      estimatorParamMaps = nbparamGrid,\n                      evaluator = nbevaluator,\n                      numFolds = 5)\n#fit the model to the data\nnbcvModel = nbcv.fit(train)\nprint(nbcvModel)\n\nnbpredictions = nbcvModel.transform(test)\n\nprint('Accuracy:', lrevaluator.evaluate(lrpredictions))\nprint('AUC:', BinaryClassificationMetrics(lrpredictions['label','prediction'].rdd).areaUnderROC)\nprint('PR:', BinaryClassificationMetrics(lrpredictions['label','prediction'].rdd).areaUnderPR)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"49d52fe5-3e8f-4cde-8957-caf318f14d04","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"CrossValidatorModel_e167637407f3\nAccuracy: 0.9040797820470264\nAUC: 0.8080818789687665\nPR: 0.5114596617470211\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["CrossValidatorModel_e167637407f3\nAccuracy: 0.9040797820470264\nAUC: 0.8080818789687665\nPR: 0.5114596617470211\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n#initialize Decision tree object\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n\ndtparamGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [2, 5, 10])\n             .addGrid(dt.maxBins, [10, 20])\n             .build())\n\ndtevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n\n# Create 5-fold CrossValidator\ndtcv = CrossValidator(estimator = dt,\n                      estimatorParamMaps = dtparamGrid,\n                      evaluator = dtevaluator,\n                      numFolds = 5)\n\ndtcvModel = dtcv.fit(train)\nprint(dtcvModel)\n\ndtpredictions = dtcvModel.transform(test)\n\nprint('Accuracy:', dtevaluator.evaluate(dtpredictions))\nprint('AUC:', BinaryClassificationMetrics(dtpredictions['label','prediction'].rdd).areaUnderROC)\nprint('PR:', BinaryClassificationMetrics(dtpredictions['label','prediction'].rdd).areaUnderPR)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0a6b4855-34d9-446a-a4b2-35fbdfd1fff0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"CrossValidatorModel_e77eee1dbeb5\nAccuracy: 0.7459087142411932\nAUC: 0.8075382755086205\nPR: 0.5311362363870251\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["CrossValidatorModel_e77eee1dbeb5\nAccuracy: 0.7459087142411932\nAUC: 0.8075382755086205\nPR: 0.5311362363870251\n"]}}],"execution_count":0},{"cell_type":"code","source":["##Random Forest\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\nrfparamGrid = (ParamGridBuilder().addGrid(rf.maxDepth, [2, 5, 10]).addGrid(rf.maxBins, [5, 10, 20]).addGrid(rf.numTrees, [5, 20, 50]).build())\nrfevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n\n# Create 5-fold CrossValidator\nrfcv = CrossValidator(estimator = rf,\n                      estimatorParamMaps = rfparamGrid,\n                      evaluator = rfevaluator,\n                      numFolds = 5)\nrfcvModel = rfcv.fit(train)\nprint(rfcvModel)\nrfpredictions = rfcvModel.transform(test)\nprint('RMSE:', rfevaluator.evaluate(rfpredictions))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b715d3ef-5162-41fe-a858-52b67fcb3984","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"CrossValidatorModel_fafc39eb6435\nRMSE: 0.9090756099767751\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["CrossValidatorModel_fafc39eb6435\nRMSE: 0.9090756099767751\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\ngb = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n\ngbparamGrid = (ParamGridBuilder()\n             .addGrid(gb.maxDepth, [2, 5, 10])\n             .addGrid(gb.maxBins, [10, 20, 40])\n             .addGrid(gb.maxIter, [5, 10, 20])\n             .build())\n\ngbevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n\n# Create 5-fold CrossValidator\ngbcv = CrossValidator(estimator = gb,\n                      estimatorParamMaps = gbparamGrid,\n                      evaluator = gbevaluator,\n                      numFolds = 5)\n\ngbcvModel = gbcv.fit(train)\nprint(gbcvModel)\n\ngbpredictions = gbcvModel.transform(test)\n\nprint('Accuracy:', gbevaluator.evaluate(gbpredictions))\nprint('AUC:', BinaryClassificationMetrics(gbpredictions['label','prediction'].rdd).areaUnderROC)\nprint('PR:', BinaryClassificationMetrics(gbpredictions['label','prediction'].rdd).areaUnderPR)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1d89fdde-bb67-4b76-8009-9a51da3684bf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"CrossValidatorModel_c3696e2fa37e\nAccuracy: 0.9156455363156804\nAUC: 0.8302687102073987\nPR: 0.5246391724649102\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["CrossValidatorModel_c3696e2fa37e\nAccuracy: 0.9156455363156804\nAUC: 0.8302687102073987\nPR: 0.5246391724649102\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Section 3b) Evaluate the Model(s)\n\n1. Evaluate the models and pick which of the algorithms worked the best and why.\n\n2. Report the best model and its parameters below."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d20bb66a-f8da-4af4-9980-1f0460306125","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["for model in [\"lrpredictions\", \"dtpredictions\", \"rfpredictions\", \"nbpredictions\", \"gbpredictions\"]:\n    df = globals()[model]\n    \n    tp = df[(df.label == 1) & (df.prediction == 1)].count()\n    tn = df[(df.label == 0) & (df.prediction == 0)].count()\n    fp = df[(df.label == 0) & (df.prediction == 1)].count()\n    fn = df[(df.label == 1) & (df.prediction == 0)].count()\n    a = ((tp + tn)/df.count())\n    \n    if(tp + fn == 0.0):\n        r = 0.0\n        p = float(tp) / (tp + fp)\n    elif(tp + fp == 0.0):\n        r = float(tp) / (tp + fn)\n        p = 0.0\n    else:\n        r = float(tp) / (tp + fn)\n        p = float(tp) / (tp + fp)\n    \n    if(p + r == 0):\n        f1 = 0\n    else:\n        f1 = 2 * ((p * r)/(p + r))\n    \n    print(\"Model:\", model)\n    print(\"True Positives:\", tp)\n    print(\"True Negatives:\", tn)\n    print(\"False Positives:\", fp)\n    print(\"False Negatives:\", fn)\n    print(\"Total:\", df.count())\n    print(\"Accuracy:\", a)\n    print(\"Recall:\", r)\n    print(\"Precision: \", p)\n    print(\"F1 score:\", f1)\n    print('AUC:', BinaryClassificationMetrics(df['label','prediction'].rdd).areaUnderROC)\nprint(\"\\n\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cd728fd9-8f94-4e35-a12b-126c931fc36c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Model: lrpredictions\nTrue Positives: 1308\nTrue Negatives: 7005\nFalse Positives: 450\nFalse Negatives: 1027\nTotal: 9790\nAccuracy: 0.8491317671092952\nRecall: 0.5601713062098501\nPrecision:  0.7440273037542662\nF1 score: 0.6391399951136086\nAUC: 0.8080818789687665\nModel: dtpredictions\nTrue Positives: 1361\nTrue Negatives: 6971\nFalse Positives: 484\nFalse Negatives: 974\nTotal: 9790\nAccuracy: 0.8510725229826354\nRecall: 0.5828693790149893\nPrecision:  0.737669376693767\nF1 score: 0.6511961722488038\nAUC: 0.8075382755086205\nModel: rfpredictions\nTrue Positives: 1248\nTrue Negatives: 7104\nFalse Positives: 351\nFalse Negatives: 1087\nTotal: 9790\nAccuracy: 0.8531154239019407\nRecall: 0.5344753747323341\nPrecision:  0.7804878048780488\nF1 score: 0.6344687341128622\nAUC: 0.8238905878254242\nModel: nbpredictions\nTrue Positives: 558\nTrue Negatives: 7136\nFalse Positives: 319\nFalse Negatives: 1777\nTotal: 9790\nAccuracy: 0.7859039836567926\nRecall: 0.23897216274089936\nPrecision:  0.636259977194983\nF1 score: 0.34744707347447074\nAUC: 0.7184441364713836\nModel: gbpredictions\nTrue Positives: 1324\nTrue Negatives: 7093\nFalse Positives: 362\nFalse Negatives: 1011\nTotal: 9790\nAccuracy: 0.8597548518896834\nRecall: 0.5670235546038543\nPrecision:  0.7852906287069988\nF1 score: 0.6585426510818204\nAUC: 0.8302687102073987\n\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Model: lrpredictions\nTrue Positives: 1308\nTrue Negatives: 7005\nFalse Positives: 450\nFalse Negatives: 1027\nTotal: 9790\nAccuracy: 0.8491317671092952\nRecall: 0.5601713062098501\nPrecision:  0.7440273037542662\nF1 score: 0.6391399951136086\nAUC: 0.8080818789687665\nModel: dtpredictions\nTrue Positives: 1361\nTrue Negatives: 6971\nFalse Positives: 484\nFalse Negatives: 974\nTotal: 9790\nAccuracy: 0.8510725229826354\nRecall: 0.5828693790149893\nPrecision:  0.737669376693767\nF1 score: 0.6511961722488038\nAUC: 0.8075382755086205\nModel: rfpredictions\nTrue Positives: 1248\nTrue Negatives: 7104\nFalse Positives: 351\nFalse Negatives: 1087\nTotal: 9790\nAccuracy: 0.8531154239019407\nRecall: 0.5344753747323341\nPrecision:  0.7804878048780488\nF1 score: 0.6344687341128622\nAUC: 0.8238905878254242\nModel: nbpredictions\nTrue Positives: 558\nTrue Negatives: 7136\nFalse Positives: 319\nFalse Negatives: 1777\nTotal: 9790\nAccuracy: 0.7859039836567926\nRecall: 0.23897216274089936\nPrecision:  0.636259977194983\nF1 score: 0.34744707347447074\nAUC: 0.7184441364713836\nModel: gbpredictions\nTrue Positives: 1324\nTrue Negatives: 7093\nFalse Positives: 362\nFalse Negatives: 1011\nTotal: 9790\nAccuracy: 0.8597548518896834\nRecall: 0.5670235546038543\nPrecision:  0.7852906287069988\nF1 score: 0.6585426510818204\nAUC: 0.8302687102073987\n\n\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Section 4) Save Your Transformation Pipeline and Model(s)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c08e919-8234-444a-80e5-4131e4539d26","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Save the pipeline\npipelineModel.save(\"/mnt/epsilon/rosh/extra/pipeline\")\ndisplay(dbutils.fs.ls(\"/mnt/epsilon/rosh/extra/pipeline\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aad58ccf-bb39-4395-929f-13797841dc53","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/mnt/epsilon/rosh/extra/pipeline/metadata/","metadata/",0,0],["dbfs:/mnt/epsilon/rosh/extra/pipeline/stages/","stages/",0,0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/epsilon/rosh/extra/pipeline/metadata/</td><td>metadata/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/epsilon/rosh/extra/pipeline/stages/</td><td>stages/</td><td>0</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#save the models\nlrcvModel.save(\"/mnt/epsilon/rosh/extra/models/LogisticRegression\")\nnbcvModel.save(\"/mnt/epsilon/rosh/extra/models/NaiveBayes\")\ndtcvModel.save(\"/mnt/epsilon/rosh/extra/models/DecisionTree\")\nrfcvModel.save(\"/mnt/epsilon/rosh/extra/models/RandomForest\")\ngbcvModel.save(\"/mnt/epsilon/rosh/extra/models/GBTree\")\n\ndisplay(dbutils.fs.ls(\"/mnt/epsilon/rosh/extra/models\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0a1ecad6-6701-46a9-8cb6-c7dc77797783","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/mnt/epsilon/rosh/extra/models/DecisionTree/","DecisionTree/",0,0],["dbfs:/mnt/epsilon/rosh/extra/models/GBTree/","GBTree/",0,0],["dbfs:/mnt/epsilon/rosh/extra/models/LogisticRegression/","LogisticRegression/",0,0],["dbfs:/mnt/epsilon/rosh/extra/models/NaiveBayes/","NaiveBayes/",0,0],["dbfs:/mnt/epsilon/rosh/extra/models/RandomForest/","RandomForest/",0,0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/epsilon/rosh/extra/models/DecisionTree/</td><td>DecisionTree/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/epsilon/rosh/extra/models/GBTree/</td><td>GBTree/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/epsilon/rosh/extra/models/LogisticRegression/</td><td>LogisticRegression/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/epsilon/rosh/extra/models/NaiveBayes/</td><td>NaiveBayes/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/epsilon/rosh/extra/models/RandomForest/</td><td>RandomForest/</td><td>0</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Trying other models","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3128781507651147}},"nbformat":4,"nbformat_minor":0}
